{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "TensorFlow version: 2.19.0\n",
      "Devices available: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "GPU available: False\n",
      "→ Running on CPU only 💻\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ✅ CPU-friendly TensorFlow setup\n",
    "# ==========================================\n",
    "\n",
    "# Enable inline plotting in Jupyter notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# ---- Environment setup ----\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"        # Force CPU only (ignore GPUs)\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"       # Use TensorFlow backend\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"1\"        # Enable optimized CPU kernels\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"              # Limit parallel threads\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"4\"              # Keep CPU responsive\n",
    "\n",
    "# ---- Core libraries ----\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- Scikit-learn utilities ----\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# ---- Image utilities ----\n",
    "import matplotlib.image as mpimg\n",
    "from scipy import ndimage\n",
    "\n",
    "# ---- TensorFlow / Keras ----\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, Flatten, BatchNormalization,\n",
    "    Conv2D, MaxPooling2D, Activation\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "import pickle\n",
    "import time  # for timestamping model runs\n",
    "\n",
    "# ---- Ensure TensorFlow uses CPU only ----\n",
    "for dev_type in [\"GPU\", \"TPU\", \"MPS\"]:\n",
    "    try:\n",
    "        tf.config.set_visible_devices([], dev_type)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ---- Optional: limit TensorFlow thread usage ----\n",
    "tf.config.threading.set_intra_op_parallelism_threads(4)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(2)\n",
    "tf.keras.backend.set_floatx(\"float32\")\n",
    "\n",
    "# ---- Print environment info ----\n",
    "print(\"===================================\")\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Devices available:\", tf.config.list_physical_devices())\n",
    "print(\"GPU available:\", len(tf.config.list_physical_devices('GPU')) > 0)\n",
    "print(\"→ Running on CPU only 💻\")\n",
    "print(\"===================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (10000, 32, 32, 3)\n",
      "Training labels shape: (10000, 10)\n",
      "Test data shape: (10000, 32, 32, 3)\n",
      "Test labels shape: (10000, 10)\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "def unpickle(file):\n",
    "    \"\"\"\n",
    "    Load and return the contents of a pickled CIFAR-10 batch file.\n",
    "    \n",
    "    Args:\n",
    "        file (str): Path to the pickled CIFAR-10 batch file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing the CIFAR-10 batch data\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# Load CIFAR-10 training and test datasets\n",
    "training_data = unpickle('./cifar-10/data_batch_1')  # Load first training batch\n",
    "test_data = unpickle('./cifar-10/test_batch')       # Load test batch\n",
    "\n",
    "# Extract features (images) and labels from the loaded data\n",
    "X_train = training_data.get(b'data')        # Training images (flattened)\n",
    "train_labels = training_data.get(b'labels')  # Training labels\n",
    "\n",
    "X_test = test_data.get(b'data')             # Test images (flattened)\n",
    "test_labels = test_data.get(b'labels')       # Test labels\n",
    "\n",
    "# Reshape the data into the correct format for TensorFlow/Keras\n",
    "# Original shape: (num_images, 3072) where 3072 = 32*32*3 (flattened 32x32 RGB images)\n",
    "# New shape: (num_images, 32, 32, 3) - 4D tensor with (batch, height, width, channels)\n",
    "X_train = X_train.reshape((len(X_train), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "X_test = X_test.reshape((len(X_test), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "\n",
    "# Normalize pixel values to [0, 1] range for better training stability\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert class vectors to binary class matrices (one-hot encoding)\n",
    "# This transforms integer labels into a binary matrix where each row corresponds to one sample\n",
    "# and each column corresponds to a class (10 classes for CIFAR-10)\n",
    "y_train = to_categorical(train_labels, 10)  # 10 classes in CIFAR-10\n",
    "y_test = to_categorical(test_labels, 10)    # 10 classes in CIFAR-10\n",
    "\n",
    "# Print dataset information\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)\n",
    "print(\"Test labels shape:\", y_test.shape)\n",
    "print(\"Number of classes:\", y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard logs will be saved to: logs/CIFAR-10_CNN_Dropout_10_Percent_1760622610\n",
      "To view the TensorBoard, run the next cell after training the model.\n"
     ]
    }
   ],
   "source": [
    "# Create a unique name for the TensorBoard log directory with a timestamp\n",
    "# This helps in tracking different training runs separately\n",
    "model_name = 'CIFAR-10_CNN_Dropout_10_Percent_{}'.format(int(time.time()))\n",
    "\n",
    "# Set up TensorBoard callback for logging training metrics\n",
    "# log_dir: Directory where to save the log files to be parsed by TensorBoard\n",
    "# histogram_freq: Frequency (in epochs) at which to compute activation histograms\n",
    "# write_graph: Whether to visualize the graph in TensorBoard\n",
    "# write_images: Whether to write model weights to visualize as image in TensorBoard\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir='logs/{}'.format(model_name),\n",
    "    histogram_freq=1,      # Log histograms every epoch\n",
    "    write_graph=True,      # Visualize the computation graph\n",
    "    write_images=True,     # Save model weights as images\n",
    "    update_freq='epoch'    # Log metrics at the end of each epoch\n",
    ")\n",
    "\n",
    "print(f\"TensorBoard logs will be saved to: logs/{model_name}\")\n",
    "print(\"To view the TensorBoard, run the next cell after training the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.2607 - loss: 2.1786 - val_accuracy: 0.1200 - val_loss: 2.7783\n",
      "Epoch 2/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.4591 - loss: 1.5268 - val_accuracy: 0.1370 - val_loss: 3.6643\n",
      "Epoch 3/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.5228 - loss: 1.3425 - val_accuracy: 0.1315 - val_loss: 3.5035\n",
      "Epoch 4/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.5568 - loss: 1.2438 - val_accuracy: 0.3480 - val_loss: 1.9294\n",
      "Epoch 5/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.5988 - loss: 1.1260 - val_accuracy: 0.4680 - val_loss: 1.5579\n",
      "Epoch 6/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.6387 - loss: 1.0565 - val_accuracy: 0.4785 - val_loss: 1.5421\n",
      "Epoch 7/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.6512 - loss: 0.9536 - val_accuracy: 0.5320 - val_loss: 1.3573\n",
      "Epoch 8/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6740 - loss: 0.9132 - val_accuracy: 0.5665 - val_loss: 1.2514\n",
      "Epoch 9/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.6980 - loss: 0.8431 - val_accuracy: 0.5765 - val_loss: 1.2266\n",
      "Epoch 10/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7113 - loss: 0.8109 - val_accuracy: 0.5530 - val_loss: 1.3718\n",
      "Epoch 11/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7325 - loss: 0.7425 - val_accuracy: 0.4960 - val_loss: 1.6544\n",
      "Epoch 12/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7527 - loss: 0.7075 - val_accuracy: 0.5910 - val_loss: 1.2386\n",
      "Epoch 13/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7732 - loss: 0.6453 - val_accuracy: 0.5765 - val_loss: 1.2902\n",
      "Epoch 14/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7839 - loss: 0.6169 - val_accuracy: 0.5845 - val_loss: 1.3188\n",
      "Epoch 15/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8025 - loss: 0.5855 - val_accuracy: 0.5815 - val_loss: 1.3628\n",
      "Epoch 16/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8130 - loss: 0.5482 - val_accuracy: 0.5980 - val_loss: 1.2910\n",
      "Epoch 17/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8226 - loss: 0.5140 - val_accuracy: 0.5605 - val_loss: 1.4659\n",
      "Epoch 18/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.8235 - loss: 0.5099 - val_accuracy: 0.5795 - val_loss: 1.3792\n",
      "Epoch 19/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.8323 - loss: 0.4914 - val_accuracy: 0.5790 - val_loss: 1.3844\n",
      "Epoch 20/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.8420 - loss: 0.4546 - val_accuracy: 0.5950 - val_loss: 1.3542\n",
      "Epoch 21/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8463 - loss: 0.4405 - val_accuracy: 0.5980 - val_loss: 1.3956\n",
      "Epoch 22/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8490 - loss: 0.4341 - val_accuracy: 0.5820 - val_loss: 1.4463\n",
      "Epoch 23/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8511 - loss: 0.4088 - val_accuracy: 0.5835 - val_loss: 1.5098\n",
      "Epoch 24/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.8697 - loss: 0.3803 - val_accuracy: 0.5975 - val_loss: 1.4371\n",
      "Epoch 25/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.8628 - loss: 0.3817 - val_accuracy: 0.5905 - val_loss: 1.4818\n",
      "Epoch 26/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8639 - loss: 0.3762 - val_accuracy: 0.5600 - val_loss: 1.6769\n",
      "Epoch 27/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.8721 - loss: 0.3567 - val_accuracy: 0.5960 - val_loss: 1.5740\n",
      "Epoch 28/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8822 - loss: 0.3376 - val_accuracy: 0.6040 - val_loss: 1.4727\n",
      "Epoch 29/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8966 - loss: 0.3068 - val_accuracy: 0.5975 - val_loss: 1.5817\n",
      "Epoch 30/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.8945 - loss: 0.3087 - val_accuracy: 0.5730 - val_loss: 1.6822\n"
     ]
    }
   ],
   "source": [
    "# Neural network parameters\n",
    "#-----------------------------------------------\n",
    "#-----------------------------------------------\n",
    "batch_size = 64 # \n",
    "epochs = 30 # \n",
    "dropoutRate = 0.1\n",
    "#-----------------------------------------------\n",
    "#-----------------------------------------------\n",
    "num_classes = 10\n",
    "img_rows, img_cols = 32, 32\n",
    "input_shape = (img_rows, img_cols,3)\n",
    "\n",
    "# Model\n",
    "model = Sequential()\n",
    "#-----------------------------------------------\n",
    "#-----------------------------------------------\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu',input_shape=input_shape)) \n",
    "#-----------------------------------------------\n",
    "#-----------------------------------------------\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#-----------------------------------------------\n",
    "#-----------------------------------------------\n",
    "model.add(Dropout(dropoutRate)) # Value between 0 and 1 \n",
    "#-----------------------------------------------\n",
    "model.add(BatchNormalization())\n",
    "#-----------------------------------------------\n",
    "model.add(Conv2D(32, (3, 3), activation='relu')) \n",
    "#-----------------------------------------------\n",
    "#-----------------------------------------------\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#-----------------------------------------------\n",
    "#-----------------------------------------------\n",
    "model.add(Dropout(dropoutRate)) # Value between 0 and 1 \n",
    "#-----------------------------------------------\n",
    "model.add(BatchNormalization())\n",
    "#-----------------------------------------------\n",
    "#-----------------------------------------------\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(50, activation='relu')) \n",
    "#-----------------------------------------------\n",
    "#-----------------------------------------------\n",
    "model.add(Dropout(dropoutRate)) # Value between 0 and 1 \n",
    "#-----------------------------------------------\n",
    "#-----------------------------------------------\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Trainen van het CNN\n",
    "history = model.fit(X_train, y_train,batch_size=batch_size, epochs=epochs, validation_split=0.2, verbose=1, callbacks = [tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second model\n",
    "\n",
    "\n",
    "NAME = 'CIFAR-10_CNN_Dropout_30_Percent'.format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.1866 - loss: 2.4736 - val_accuracy: 0.1225 - val_loss: 2.9405\n",
      "Epoch 2/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.3352 - loss: 1.8492 - val_accuracy: 0.1100 - val_loss: 3.5161\n",
      "Epoch 3/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.4017 - loss: 1.6800 - val_accuracy: 0.2170 - val_loss: 2.8671\n",
      "Epoch 4/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.4364 - loss: 1.5556 - val_accuracy: 0.3470 - val_loss: 1.9246\n",
      "Epoch 5/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.4629 - loss: 1.4930 - val_accuracy: 0.3995 - val_loss: 1.7825\n",
      "Epoch 6/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.4785 - loss: 1.4377 - val_accuracy: 0.3360 - val_loss: 1.9108\n",
      "Epoch 7/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.4976 - loss: 1.3946 - val_accuracy: 0.3320 - val_loss: 1.9064\n",
      "Epoch 8/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.5112 - loss: 1.3463 - val_accuracy: 0.5300 - val_loss: 1.3122\n",
      "Epoch 9/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.5335 - loss: 1.2948 - val_accuracy: 0.5350 - val_loss: 1.3019\n",
      "Epoch 10/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.5425 - loss: 1.2640 - val_accuracy: 0.5115 - val_loss: 1.3574\n",
      "Epoch 11/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.5494 - loss: 1.2418 - val_accuracy: 0.5310 - val_loss: 1.3496\n",
      "Epoch 12/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.5745 - loss: 1.1880 - val_accuracy: 0.4955 - val_loss: 1.4344\n",
      "Epoch 13/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.5771 - loss: 1.1872 - val_accuracy: 0.5615 - val_loss: 1.2166\n",
      "Epoch 14/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.5949 - loss: 1.1228 - val_accuracy: 0.4595 - val_loss: 1.5703\n",
      "Epoch 15/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.5997 - loss: 1.1227 - val_accuracy: 0.5785 - val_loss: 1.2105\n",
      "Epoch 16/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.6165 - loss: 1.0832 - val_accuracy: 0.5080 - val_loss: 1.4203\n",
      "Epoch 17/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.6183 - loss: 1.0679 - val_accuracy: 0.5335 - val_loss: 1.3542\n",
      "Epoch 18/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.6233 - loss: 1.0284 - val_accuracy: 0.5840 - val_loss: 1.1565\n",
      "Epoch 19/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.6263 - loss: 1.0478 - val_accuracy: 0.4075 - val_loss: 1.8784\n",
      "Epoch 20/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.6346 - loss: 1.0228 - val_accuracy: 0.5670 - val_loss: 1.2662\n",
      "Epoch 21/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.6422 - loss: 0.9787 - val_accuracy: 0.6010 - val_loss: 1.1330\n",
      "Epoch 22/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.6428 - loss: 1.0065 - val_accuracy: 0.5985 - val_loss: 1.1370\n",
      "Epoch 23/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.6484 - loss: 0.9594 - val_accuracy: 0.5265 - val_loss: 1.4519\n",
      "Epoch 24/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.6540 - loss: 0.9593 - val_accuracy: 0.5985 - val_loss: 1.1516\n",
      "Epoch 25/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.6639 - loss: 0.9178 - val_accuracy: 0.6155 - val_loss: 1.0952\n",
      "Epoch 26/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.6576 - loss: 0.9351 - val_accuracy: 0.5995 - val_loss: 1.1372\n",
      "Epoch 27/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.6714 - loss: 0.9060 - val_accuracy: 0.6000 - val_loss: 1.1500\n",
      "Epoch 28/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.6664 - loss: 0.9174 - val_accuracy: 0.5950 - val_loss: 1.1731\n",
      "Epoch 29/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.6674 - loss: 0.9180 - val_accuracy: 0.6045 - val_loss: 1.1522\n",
      "Epoch 30/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.6716 - loss: 0.9089 - val_accuracy: 0.5300 - val_loss: 1.4265\n"
     ]
    }
   ],
   "source": [
    "# Neural network parameters\n",
    "#-----------------------------------------------\n",
    "#-----------------------------------------------\n",
    "batch_size = 64 # \n",
    "epochs = 30 # \n",
    "dropoutRate = 0.3\n",
    "#-----------------------------------------------\n",
    "#-----------------------------------------------\n",
    "num_classes = 10\n",
    "img_rows, img_cols = 32, 32\n",
    "input_shape = (img_rows, img_cols,3)\n",
    "\n",
    "# Model\n",
    "model = Sequential()\n",
    "#-----------------------------------------------\n",
    "#-----------------------------------------------\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu',input_shape=input_shape)) \n",
    "#-----------------------------------------------\n",
    "#-----------------------------------------------\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#-----------------------------------------------\n",
    "#-----------------------------------------------\n",
    "model.add(Dropout(dropoutRate)) # Value between 0 and 1 \n",
    "#-----------------------------------------------\n",
    "model.add(BatchNormalization())\n",
    "#-----------------------------------------------\n",
    "model.add(Conv2D(32, (3, 3), activation='relu')) \n",
    "#-----------------------------------------------\n",
    "#-----------------------------------------------\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#-----------------------------------------------\n",
    "#-----------------------------------------------\n",
    "model.add(Dropout(dropoutRate)) # Value between 0 and 1 \n",
    "#-----------------------------------------------\n",
    "model.add(BatchNormalization())\n",
    "#-----------------------------------------------\n",
    "#-----------------------------------------------\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(50, activation='relu')) \n",
    "#-----------------------------------------------\n",
    "#-----------------------------------------------\n",
    "model.add(Dropout(dropoutRate)) # Value between 0 and 1 \n",
    "#-----------------------------------------------\n",
    "#-----------------------------------------------\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Trainen van het CNN\n",
    "history = model.fit(X_train, y_train,batch_size=batch_size, epochs=epochs, validation_split=0.2, verbose=1, callbacks = [tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ce3667125ffd0204\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ce3667125ffd0204\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further reading on tensorboard:\n",
    "https://www.tensorflow.org/tensorboard/get_started "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
